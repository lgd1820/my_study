# RoBERTa(Robustly Optimized BERT pre-training Approach)

RoBERTa는 기본적으로 BERT와 동일하고 사전 학습 시 다음의 항목을 변경하였다.
- MLM 태스크에서 정적 마스킹이 아닌 동적 마스킹 방법을 적용
- NSP 태스크를 제거하고 MLM 태스크만 학습에 사용
- 배치 크기를 증가해 학습
- 토크나이저로 BBPE(byte-level BPE)를 사용

## 1. 정적 마스크 대신 동적 마스크 사용

기존 BERT 모델에서는 마스크된 토큰의 위치가 같았지만 RoBERTa에서는 하나의 문장을 10개로 복사하여 무작위로 15% 확률로 마스크 작업을 수행한다. 그럼 10개의 문장은 각기 다른 마스크된 토큰을 가지게 된다.

모델 학습 시 에폭이 다를 때마다 문장의 다른 버전을 넣어준다. 예를들어 에폭이 40이고 문장이 버전이 10개가 있으면 에폭 1, 11, 21, 31에는 문장 버전 1을 에폭 2, 12, 22, 32에는 문장 버전 2를 모델 학습에 사용한다.

## 2. NSP 태스크 제거

RoBERTa의 연구원들은 NSP 태스크가 BERT 모델 사전 학습에 유용하지 않다는 사실을 발견해 MLM 태스크만 사용한다. NSP 태스크의 중요성을 이해하기 위해 다음과 같은 실험을 진행했다.
- SEGMENT-PAIR + NSP : NSP를 사용해 BERT를 학습시킨다. 이는 원래 BERT 모델 학습 방법과 유사하며, 입력은 512개 이하의 토큰 쌍으로 구성한다.
- SENTENCE-PAIR + NSP : 위와 마찬가지로 NSP를 이용해 학습한다. 이때 입력값은 한 문서의 연속된 부분 또는 다른 문서에서 추출한 문장을 쌍으로 구성하며, 512개 이하의 토큰 쌍을 입력한다.
- FULL SENTENCES : NSP를 사용하지 않고 BERT를 학습시킨다. 여기서 입력값은 하나 이상의 문서에서 지속적으로 샘플링한 결과를 사용한다. 입력 토큰은 512 토큰이다. 하나의 문서 마지막까지 샘플링을 한 이후에는 다음 문서에서 샘플링 작업을 이어간다.
- DOC SENTENCES : NSP를 사용하지 않고 BERT를 학습시킨다. FULL SENTENCES 와 전체적으로 유사하나, 입력값은 하나의 문서에서만 샘플링한 결과만 입력한다. 하나의 문서 마지막까지 샘플링을 한 후에 다음 문서 내용을 사용하지 않는다.

|모델|SQuAD 1.1/2.0|MNLI-m|SST-2|RACE|
|---|----|---|--|---|
|SEGMENT-PAIR|90.4/78.7|84.0|92.9|64.2|
|SENTENCE-PAIR|88.7/76.2|82.9|92.1|63.0|
|FULL SENTENCES|90.4/79.1|84.7|92.5|64.8|
|DOC SENTENCES|90.6/79.7|84.7|92.7|65.6|

출처 : https://arxiv.org/pdf/1907.11692.pdf  
출처 : 구글 BERT의 정석