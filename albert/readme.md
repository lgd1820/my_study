# ALBERT(A Lite version of BERT)

BERT의 문제점 중 하나는 수백만 개의 변수로 구성되어있다. BERT-base 같은 경우 약 1억개의 변수로 구성되어 있어 모델 학습이 어렵고 추론 시 시간이 많이 걸린다.

ALBERT는 두 가지 방법을 사용해 BERT와 대비해 변수를 적게 사용한다.
- 크로스 레이어 변수 공유(cross-layer parameter sharing)
- 펙토라이즈 임베딩 레이어 변수화(factorized embedding layer parameterization)

## 1. 크로스 레이어 변수 공유
BERT 모델의 변수 개수를 줄이는 방법으로써 N개의 인코더로 구성되어있는 인코더 레이어에서 첫 번째 인코더 레이어의 변수만 학습한 다음 첫 번째 인코더 레이어의 변수를 다른 모든 인코더 레이어에 공유한다.

- 공유 방법
    - All-shared : 첫 번째 인코더의 하위 레이어에 있는 모든 변수를 나머지 인코더와 공유
    - Shared feedforward : 첫 번째 인코더의 피드포워드 네트워크의 변수만 다른 인코더 레이어의 피드 포워드 네트워크 공유
    - Shared attention : 첫 번째 인코더 레이어의 멀티 헤드 어텐션의 변수만 다른 인텐트 레이어와 공유

이 중에서 ALBERT는 첫 번째인 All-shared를 기본 옵션으로 사용한다.

## 2. 펙토라이즈 임베딩 변수화
워드피스 임베딩과 히든 레이어 임베딩은 모두 모델 학습이 진행될 때 학습이 이루어진다. 워드피스 임베딩 크기를 히든 레이어 임베딩 크기와 동일하게 설정하면 학습해야할 변수 늘어난다. 이런 현상을 방지하기 위해 임베딩 행렬을 더 작은 행렬로 분해하는 방법인 펙토라이즈 임베딩 변수화 방법을 사용한다.

워드피스 임베딩의 경우 사전에 원-핫 인코딩 벡터를 히든 레이어로 투영하기 때문에 히든 레이어 임베딩 크기와 동일하게 설정했다. 행렬 분해를 사용하면, 원-핫 인코딩된 어휘 벡터를 히든 공간(V x H)에 직접 투영하는 대신 처음에는 원-핫 인코딩된 벡터를 낮은 차원의 임베딩 공간(V x E)로 투영하고, 그다음에는 낮은 차원의 임베딩을 히든 공간(E x H)으로 투영하는 것이다.

즉 V x H 대신, V x E 와 E x H 로 분해하는 방법을 사용한다.

- 사전의 원-핫 인코딩한 벡터를 V를 저차원 워드피스 임베딩 공간 E로 투영한다 (V x E), 이때 워드피스 임베딩의 차원은 V x E = 30000 x 128이 된다.
- 그 다음 워드피스 임베딩 공간 E를 히든 레이어 H로 투영한다. 이때 차원은 E x H = 128 x 768이 된다.

## 3. ALBERT 학습 모델
