# ALBERT(A Lite version of BERT)

BERT의 문제점 중 하나는 수백만 개의 변수로 구성되어있다. BERT-base 같은 경우 약 1억개의 변수로 구성되어 있어 모델 학습이 어렵고 추론 시 시간이 많이 걸린다.

ALBERT는 두 가지 방법을 사용해 BERT와 대비해 변수를 적게 사용한다.
- 크로스 레이어 변수 공유(cross-layer parameter sharing)
- 펙토라이즈 임베딩 레이어 변수화(factorized embedding layer parameterization)

## 1. 크로스 레이어 변수 공유
BERT 모델의 변수 개수를 줄이는 방법으로써 N개의 인코더로 구성되어있는 인코더 레이어에서 첫 번째 인코더 레이어의 변수만 학습한 다음 첫 번째 인코더 레이어의 변수를 다른 모든 인코더 레이어에 공유한다.

- 공유 방법
    - All-shared : 첫 번째 인코더의 하위 레이어에 있는 모든 변수를 나머지 인코더와 공유
    - Shared feedforward : 첫 번째 인코더의 피드포워드 네트워크의 변수만 다른 인코더 레이어의 피드 포워드 네트워크 공유
    - Shared attention : 첫 번째 인코더 레이어의 멀티 헤드 어텐션의 변수만 다른 인텐트 레이어와 공유

이 중에서 ALBERT는 첫 번째인 All-shared를 기본 옵션으로 사용한다.

## 2. 펙토라이즈 임베딩 변수화
워드피스 임베딩과 히든 레이어 임베딩은 모두 모델 학습이 진행될 때 학습이 이루어진다. 워드피스 임베딩 크기를 히든 레이어 임베딩 크기와 동일하게 설정하면 학습해야할 변수 늘어난다. 이런 현상을 방지하기 위해 임베딩 행렬을 더 작은 행렬로 분해하는 방법인 펙토라이즈 임베딩 변수화 방법을 사용한다.

워드피스 임베딩의 경우 사전에 원-핫 인코딩 벡터를 히든 레이어로 투영하기 때문에 히든 레이어 임베딩 크기와 동일하게 설정했다. 행렬 분해를 사용하면, 원-핫 인코딩된 어휘 벡터를 히든 공간(V x H)에 직접 투영하는 대신 처음에는 원-핫 인코딩된 벡터를 낮은 차원의 임베딩 공간(V x E)로 투영하고, 그다음에는 낮은 차원의 임베딩을 히든 공간(E x H)으로 투영하는 것이다.

즉 V x H 대신, V x E 와 E x H 로 분해하는 방법을 사용한다.

- 사전의 원-핫 인코딩한 벡터를 V를 저차원 워드피스 임베딩 공간 E로 투영한다 (V x E), 이때 워드피스 임베딩의 차원은 V x E = 30000 x 128이 된다.
- 그 다음 워드피스 임베딩 공간 E를 히든 레이어 H로 투영한다. 이때 차원은 E x H = 128 x 768이 된다.

## 3. ALBERT 학습 모델
BERT의 경우 MLM과 NSP 태스크를 통해 사전 학습을 진행한다. ALBERT의 경우 MLM은 사용하지만 NSP 대신 문장 순서 예측(sentence order prediction)인 SOP를 사용한다.

### 문장 순서 예측
NSP 태스크는 한 쌍의 문장이 isNext 또는 notNext인지를 예측하는 형태로 학습이 이루어진다. SOP 태스크는 주어진 한 쌍의 문장이 문장 순서가 바뀌었는지 여부를 예측하도록 모델 학습이 이루어진다.

```
문장 1: She cooked pasta
문장 2: It was delicious
```

위와 같은 경우 문장 2가 문장 1 다음에 온다는 것을 알 수 있으므로 positive가 된다.

## 4. ALBERT
|모델|파라미터|레이어|히든|임베딩|
|---|----|---|--|---|
|BERT-base|110M|12|768|768|
|BERT-large|334M|24|1024|1024|
|ALBERT-base|12M|12|768|128|
|ALBERT-large|18M|24|1024|128|
|ALBERT-xlarge|60M|24|2048|128|
|ALBERT-xxlarge|235M|12|4096|128|

모든 경우에서 ALBERT는 BERT보다 더 적은 변수를 사용한다. ALBERT는 BERT의 대안으로 사용하기 좋은 모델이다.

ALBERT-xxlarge 같은 경우에는 sQuaAD1.1, sQuaAD12.0, MLNI, STT-2, RACE 등의 자연어 태스크에서 BERT 보다 월등한 상향을 보인다.

논문 : https://arxiv.org/pdf/1909.11942.pdf
출처 : 구글 BERT의 정석